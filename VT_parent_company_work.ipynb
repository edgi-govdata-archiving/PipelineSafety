{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7db4222a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\ashok\\documents\\github\\pipelinesafety\\.venv\\lib\\site-packages (2.3.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\ashok\\documents\\github\\pipelinesafety\\.venv\\lib\\site-packages (from pandas) (2.3.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\ashok\\documents\\github\\pipelinesafety\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ashok\\documents\\github\\pipelinesafety\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\ashok\\documents\\github\\pipelinesafety\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ashok\\documents\\github\\pipelinesafety\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: openpyxl in c:\\users\\ashok\\documents\\github\\pipelinesafety\\.venv\\lib\\site-packages (3.1.5)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\ashok\\documents\\github\\pipelinesafety\\.venv\\lib\\site-packages (from openpyxl) (2.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting keyboard\n",
      "  Downloading keyboard-0.13.5-py3-none-any.whl.metadata (4.0 kB)\n",
      "Downloading keyboard-0.13.5-py3-none-any.whl (58 kB)\n",
      "Installing collected packages: keyboard\n",
      "Successfully installed keyboard-0.13.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# library installations if necessary, make sure you're using .venv!\n",
    "\n",
    "%pip install pandas\n",
    "%pip install openpyxl\n",
    "%pip install keyboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e90b5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import keyboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3dc0f332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PHMSA distinct cpf_key count: 4944\n",
      "GJF rows with extracted cpf_key: 718\n",
      "Matched rows: 667\n",
      "Unmatched GJF rows after cpf merge: 53\n"
     ]
    }
   ],
   "source": [
    "vt = pd.read_excel(\"ViolationTracker_21Aug2025_PHMSA_only.xlsx\")\n",
    "phmsa = pd.read_excel(\"PHMSA_RAW_DATA.xlsx\")\n",
    "\n",
    "# Helper function to make a numeric-only CPF key for PHMSA (e.g., \"42025041NOA\" -> \"42025041\")\n",
    "def phmsa_cpf_key(s):\n",
    "    if pd.isna(s):\n",
    "        return \"\"\n",
    "    m = re.match(r'\\D*(\\d+)', str(s))\n",
    "    return m.group(1) if m else \"\"\n",
    "\n",
    "phmsa[\"cpf_key\"] = phmsa[\"CPF_Number\"].apply(phmsa_cpf_key)\n",
    "\n",
    "# Helper function that extracts CPF number from \"info_source\" column in GJF data.\n",
    "# Try pattern 'cpf_123456789' first, else fallback to the longest digit run\n",
    "def vt_cpf_from_info(url):\n",
    "    if pd.isna(url):\n",
    "        return \"\"\n",
    "    txt = str(url)\n",
    "    # This regex matches things like 'cpf_12345' or 'CPF-12345'\n",
    "    m = re.search(r'(?i)cpf[_\\-]?(\\d{4,})', txt)   # case-insensitive, require >=4 digits\n",
    "    if m:\n",
    "        return m.group(1)\n",
    "    # fallback: find all digit runs and return the longest (likely the CPF if present)\n",
    "    runs = re.findall(r'(\\d{4,})', txt)   # capture runs of 4+ digits\n",
    "    if not runs:\n",
    "        return \"\"\n",
    "    # choose the longest run (if ties, first)\n",
    "    runs_sorted = sorted(runs, key=lambda x: (-len(x), x))\n",
    "    return runs_sorted[0]\n",
    "\n",
    "vt[\"cpf_key_extracted\"] = vt[\"info_source\"].apply(vt_cpf_from_info)\n",
    "\n",
    "# 3) Quick sanity counts\n",
    "print(\"PHMSA distinct cpf_key count:\", phmsa[\"cpf_key\"].nunique())\n",
    "print(\"GJF rows with extracted cpf_key:\", (vt[\"cpf_key_extracted\"] != \"\").sum())\n",
    "\n",
    "# 4) Merge VT -> PHMSA on the cpf key\n",
    "merged = vt.merge(phmsa, left_on=\"cpf_key_extracted\", right_on=\"cpf_key\", how=\"left\", suffixes=(\"_vt\", \"_phmsa\"))\n",
    "\n",
    "# 5) Inspect mismatches\n",
    "matched = merged[merged[\"CPF_Number\"].notna()]\n",
    "unmatched = merged[merged[\"CPF_Number\"].isna()]\n",
    "print(\"Matched rows:\", len(matched))\n",
    "print(\"Unmatched GJF rows after cpf merge:\", len(unmatched))\n",
    "\n",
    "# Save a sample of merged/unmatched for inspection\n",
    "# merged.to_excel(\"VT_PHMSA_merged_by_cpf.xlsx\", index=False)\n",
    "# unmatched.to_excel(\"unmatched_rows.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "421dbef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashok\\AppData\\Local\\Temp\\ipykernel_12140\\3028011393.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  matched[\"current_parent_name_n\"] = matched[\"current_parent_name\"].apply(normalize_parent)\n",
      "C:\\Users\\ashok\\AppData\\Local\\Temp\\ipykernel_12140\\3028011393.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  matched[\"reporting_date_parent_n\"] = matched[\"reporting_date_parent\"].apply(normalize_parent)\n",
      "C:\\Users\\ashok\\AppData\\Local\\Temp\\ipykernel_12140\\3028011393.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  matched[\"parent_changed\"] = matched[\"current_parent_name_n\"] != matched[\"reporting_date_parent_n\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parent change flag counts:\n",
      "parent_changed\n",
      "False    475\n",
      "True     192\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Saved 667 total rows to 'VT_PHMSA_matched_with_parentchange.xlsx'\n",
      "Saved 192 changed-parent rows to 'VT_PHMSA_parentchanged_only.xlsx'\n"
     ]
    }
   ],
   "source": [
    "# Helper to normalize parent name\n",
    "def normalize_parent(name):\n",
    "    if pd.isna(name):\n",
    "        return \"\"\n",
    "    return re.sub(r'[^a-z0-9 ]', '', str(name).lower().strip())\n",
    "\n",
    "# New columns for normalized names\n",
    "matched[\"current_parent_name_n\"] = matched[\"current_parent_name\"].apply(normalize_parent)\n",
    "matched[\"reporting_date_parent_n\"] = matched[\"reporting_date_parent\"].apply(normalize_parent)\n",
    "\n",
    "# Create parent_changed flag\n",
    "matched[\"parent_changed\"] = matched[\"current_parent_name_n\"] != matched[\"reporting_date_parent_n\"]\n",
    "print(\"Parent change flag counts:\")\n",
    "print(matched[\"parent_changed\"].value_counts())\n",
    "\n",
    "# Save full version\n",
    "matched.to_excel(\"VT_PHMSA_matched_with_parentchange.xlsx\", index=False)\n",
    "\n",
    "# Filter only changed-parent entries\n",
    "changed_only = matched[matched[\"parent_changed\"] == True].copy()\n",
    "\n",
    "# Save filtered version\n",
    "changed_only.to_excel(\"VT_PHMSA_parentchanged_only.xlsx\", index=False)\n",
    "\n",
    "print(f\"\\nSaved {len(matched)} total rows to 'VT_PHMSA_matched_with_parentchange.xlsx'\")\n",
    "print(f\"Saved {len(changed_only)} changed-parent rows to 'VT_PHMSA_parentchanged_only.xlsx'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da4aa04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5 cached acquisition dates from 'VT_PHMSA_parentchanged_with_dates.xlsx'\n",
      "'VT_PHMSA_parentchanged_with_dates.xlsx' found but missing acquisition_announcement_exact_date column; starting with empty cache.\n",
      "\n",
      "Press ESC at any time to stop and save progress.\n",
      "\n",
      "\n",
      "[1/24852] HISTORY_RECAP for DOMINION TRANSMISSION INC, old parent = dominion energy, new parent = berkshire hathaway:\n",
      "Berkshire Hathaway acquired Dominion Energy Transmission, Inc. in November 2020\n",
      "\n",
      "\n",
      "[513/24852] HISTORY_RECAP for ROCKIES EXPRESS PIPELINE LLC, old parent = tallgrass energy, new parent = blackstone:\n",
      "Blackstone acquired Tallgrass in 2019\n",
      "\n",
      "\n",
      "[856/24852] HISTORY_RECAP for AERA ENERGY LLC, old parent = shell plc, new parent = california resources:\n",
      "Royal Dutch Shell changed its name to Shell PLC in 2022. Shell sold Aera to California Resources in 2024\n",
      "\n",
      "\n",
      "[857/24852] HISTORY_RECAP for CALUMET SUPERIOR LLC, old parent = calumet specialty products, new parent = cenovus energy:\n",
      "In 2017 Calumet sold the operation to a subsidiary of Husky Energy, which in 2021 merged with Cenovus Energy.\n",
      "\n",
      "\n",
      "[921/24852] HISTORY_RECAP for HESS CORPORATION, old parent = hess corp, new parent = chevron:\n",
      "Chevron acquired Hess in 2025.\n",
      "\n",
      "\n",
      "[922/24852] HISTORY_RECAP for NOBLE MIDSTREAM SERVICES LLC, old parent = noble energy, new parent = chevron:\n",
      "Chevron bought Noble in 2020\n",
      "\n",
      "\n",
      "[930/24852] HISTORY_RECAP for WHITING PETROLEUM CORP, old parent = whiting petroleum, new parent = chord energy:\n",
      "In 2022 Whiting merged with Oasis Petroleum to form Chord Energy\n",
      "\n",
      "\n",
      "[931/24852] HISTORY_RECAP for ENERGY XXI PIPELINE LLC, old parent = energy xxi, new parent = cox oil:\n",
      "Cox Oil bought Energy XXI later in 2018\n",
      "\n",
      "\n",
      "[939/24852] HISTORY_RECAP for ALON USA LP, old parent = alon usa energy, new parent = delek:\n",
      "Delek acquired Alon USA in 2017\n",
      "\n",
      "\n",
      "[940/24852] HISTORY_RECAP for WILLIAMS ENERGY LLC, old parent = wpx energy, new parent = devon energy:\n",
      "Devon Energy bought WPX in 2021\n",
      "\n",
      "\n",
      "Escape pressed — saving progress...\n",
      "Progress saved to VT_PHMSA_parentchanged_with_dates.xlsx\n",
      "\n",
      "Final data saved to VT_PHMSA_parentchanged_with_dates.xlsx\n"
     ]
    }
   ],
   "source": [
    "INPUT_FILE = \"VT_PHMSA_parentchanged_only.xlsx\"\n",
    "OUTPUT_FILE = \"VT_PHMSA_parentchanged_with_dates.xlsx\"\n",
    "\n",
    "df = pd.read_excel(INPUT_FILE)\n",
    "\n",
    "# Ensure acquisition_exact_date column exists\n",
    "if \"acquisition_exact_date\" not in df.columns:\n",
    "    df[\"acquisition_exact_date\"] = None\n",
    "\n",
    "# Ensure acquisition_announcement_exact_date column exists\n",
    "if \"acquisition_announcement_exact_date\" not in df.columns:\n",
    "    df[\"acquisition_announcement_exact_date\"] = None\n",
    "\n",
    "acquisition_cache = {}\n",
    "acquisition_announcement_cache = {}\n",
    "\n",
    "valid_output_file = False\n",
    "# Load cache from OUTPUT_FILE if available\n",
    "if os.path.exists(OUTPUT_FILE):\n",
    "    try:\n",
    "        df_existing = pd.read_excel(OUTPUT_FILE)\n",
    "        if \"history_recap\" in df_existing.columns:\n",
    "            if \"acquisition_exact_date\" in df_existing.columns:\n",
    "                existing_with_dates = df_existing.dropna(subset=[\"acquisition_exact_date\"])\n",
    "                acquisition_cache = dict(zip(existing_with_dates[\"history_recap\"], existing_with_dates[\"acquisition_exact_date\"]))\n",
    "                print(f\"Loaded {len(acquisition_cache)} cached acquisition dates from '{OUTPUT_FILE}'\")\n",
    "                # Merge back existing acquisition dates into df\n",
    "                df = df.merge(\n",
    "                    df_existing[[\"history_recap\", \"acquisition_exact_date\"]],\n",
    "                    on=\"history_recap\",\n",
    "                    how=\"left\",\n",
    "                    suffixes=(\"\", \"_existing\")\n",
    "                )\n",
    "                df[\"acquisition_exact_date\"] = df[\"acquisition_exact_date\"].combine_first(df[\"acquisition_exact_date_existing\"])\n",
    "                df.drop(columns=[\"acquisition_exact_date_existing\"], inplace=True)\n",
    "            else:\n",
    "                print(f\"'{OUTPUT_FILE}' found but missing acquisition_exact_date column; starting with empty cache.\")\n",
    "            if \"acquisition_announcement_exact_date\" in df_existing.columns:\n",
    "                existing_with_dates = df_existing.dropna(subset=[\"acquisition_announcement_exact_date\"])\n",
    "                acquisition_announcement_cache = dict(zip(existing_with_dates[\"history_recap\"], existing_with_dates[\"acquisition_announcement_exact_date\"]))\n",
    "                print(f\"Loaded {len(acquisition_announcement_cache)} cached acquisition announcement dates from '{OUTPUT_FILE}'\")\n",
    "                # Merge back existing acquisition dates into df\n",
    "                df = df.merge(\n",
    "                    df_existing[[\"history_recap\", \"acquisition_announcement_exact_date\"]],\n",
    "                    on=\"history_recap\",\n",
    "                    how=\"left\",\n",
    "                    suffixes=(\"\", \"_existing\")\n",
    "                )\n",
    "                df[\"acquisition_announcement_exact_date\"] = df[\"acquisition_announcement_exact_date\"].combine_first(df[\"acquisition_announcement_exact_date_existing\"])\n",
    "                df.drop(columns=[\"acquisition_announcement_exact_date\"], inplace=True)\n",
    "            else:\n",
    "                print(f\"'{OUTPUT_FILE}' found but missing acquisition_announcement_exact_date column; starting with empty cache.\")\n",
    "        else:\n",
    "            print(f\"WARNING: '{OUTPUT_FILE}' found but missing history_recap; starting with empty cache.\")\n",
    "        valid_output_file = True\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading '{OUTPUT_FILE}': {e}\")\n",
    "else:\n",
    "    print(\"No existing output file found, quitting.\")\n",
    "\n",
    "if valid_output_file:\n",
    "    # Interactive acquisition date input\n",
    "    print(\"\\nPress ESC at any time to stop and save progress.\\n\")\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        # Check for keyboard escape before each iteration\n",
    "        if keyboard.is_pressed(\"esc\"):\n",
    "            print(\"\\nEscape pressed — saving progress...\")\n",
    "            break\n",
    "        \n",
    "        old_parent = str(row[\"reporting_date_parent_n\"])\n",
    "        new_parent = str(row[\"current_parent_name_n\"])\n",
    "        company_name = str(row[\"company\"])\n",
    "        recap = str(row[\"history_recap\"])\n",
    "        current_value = row.get(\"acquisition_exact_date\", None)\n",
    "        current_announcement_value = row.get(\"acquisition_announcement_exact_date\", None)\n",
    "\n",
    "        # Skip rows that already have a cached or filled date\n",
    "        if recap in acquisition_cache and recap in acquisition_announcement_cache:\n",
    "            df.at[idx, \"acquisition_exact_date\"] = acquisition_cache[recap]\n",
    "            df.at[idx, \"acquisition_announcement_exact_date\"] = acquisition_announcement_cache[recap]\n",
    "            continue\n",
    "        if pd.notna(current_value) and pd.notna(current_announcement_value):\n",
    "            acquisition_cache[recap] = current_value\n",
    "            acquisition_announcement_cache[recap] = current_announcement_value\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n[{idx+1}/{len(df)}] HISTORY_RECAP for {company_name}, old parent = {old_parent}, new parent = {new_parent}:\\n{recap}\\n\")\n",
    "        user_input = input(\"Enter acquisition exact date (MM/DD/YYYY) or press Enter to skip: \").strip()\n",
    "\n",
    "        if user_input:\n",
    "            df.at[idx, \"acquisition_exact_date\"] = user_input\n",
    "            acquisition_cache[recap] = user_input\n",
    "        \n",
    "        user_input = input(\"Enter acquisition ANNOUNCEMENT exact date (MM/DD/YYYY) or press Enter to skip: \").strip()\n",
    "\n",
    "        if user_input:\n",
    "            df.at[idx, \"acquisition_announcement_exact_date\"] = user_input\n",
    "            acquisition_announcement_cache[recap] = user_input\n",
    "\n",
    "    # Save final state after loop\n",
    "    df.to_excel(OUTPUT_FILE, index=False)\n",
    "    print(f\"\\nFinal data saved to {OUTPUT_FILE}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
